{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Term Extraction from Russian Texts\n",
    "\n",
    "This project aims to extract key terms from Russian texts using NLP techniques\n",
    "and a neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install nltk spacy scikit-learn tensorflow pandas matplotlib seaborn wordcloud pymorphy2\n",
    "\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Download spaCy Russian language model\n",
    "!python -m spacy download ru_core_news_sm\n",
    "\n",
    "print(\"Setup Complete. Libraries installed and resources downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "import string\n",
    "\n",
    "# Initialize MorphAnalyzer for Russian\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# Russian stopwords and punctuation\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "# --- Tokenization ---\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenizes Russian text.\"\"\"\n",
    "    tokens = word_tokenize(text.lower(), language='russian') # Convert to lowercase for consistency\n",
    "    return tokens\n",
    "\n",
    "# --- Lemmatization ---\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatizes a list of tokens using pymorphy2.\"\"\"\n",
    "    lemmatized_tokens = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# --- Stop Word Removal ---\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Removes Russian stopwords and punctuation from a list of tokens.\"\"\"\n",
    "    filtered_tokens = [token for token in tokens \n",
    "                       if token not in russian_stopwords \n",
    "                       and token not in punctuations \n",
    "                       and token.strip()] # Also remove empty strings that might result from punctuation removal\n",
    "    return filtered_tokens\n",
    "\n",
    "# --- Morphological Analysis ---\n",
    "def get_morphological_analysis(tokens):\n",
    "    \"\"\"Performs morphological analysis on a list of tokens.\"\"\"\n",
    "    morph_info = []\n",
    "    for token in tokens:\n",
    "        parsed_token = morph.parse(token)[0]\n",
    "        morph_info.append({'token': token, 'tag': parsed_token.tag})\n",
    "    return morph_info\n",
    "\n",
    "print(\"Text preprocessing and morphological analysis functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage of Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "sample_text = \"Пример текста для демонстрации предобработки данных, включая знаки препинания!\"\n",
    "print(f\"Original text: {sample_text}\")\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenize_text(sample_text)\n",
    "print(f\"\\nTokens: {tokens}\")\n",
    "\n",
    "# Lemmatize\n",
    "lemmatized = lemmatize_tokens(tokens)\n",
    "print(f\"\\nLemmatized: {lemmatized}\")\n",
    "\n",
    "# Remove Stopwords and Punctuation\n",
    "no_stopwords = remove_stopwords(lemmatized)\n",
    "print(f\"\\nWithout stopwords and punctuation: {no_stopwords}\")\n",
    "\n",
    "# Example with text that might produce empty strings after processing\n",
    "sample_text_2 = \"Ещё один пример - очень короткий.\"\n",
    "print(f\"\\nOriginal text 2: {sample_text_2}\")\n",
    "tokens_2 = tokenize_text(sample_text_2)\n",
    "lemmatized_2 = lemmatize_tokens(tokens_2)\n",
    "no_stopwords_2 = remove_stopwords(lemmatized_2)\n",
    "print(f\"Without stopwords (Text 2): {no_stopwords_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage of Morphological Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage for Morphological Analysis\n",
    "sample_tokens_for_morph = ['текста', 'для', 'демонстрации', 'обработки'] # Using some tokens from the previous example\n",
    "print(f\"Sample tokens for morphological analysis: {sample_tokens_for_morph}\")\n",
    "morphological_data = get_morphological_analysis(sample_tokens_for_morph)\n",
    "for item in morphological_data:\n",
    "    print(f\"Token: {item['token']}, Morphology: {item['tag']}\")\n",
    "\n",
    "print(\"\\n---\")\n",
    "\n",
    "# Example integrating with tokenize_text\n",
    "sample_text_morph = \"Красивая мама мыла раму.\"\n",
    "print(f\"\\nOriginal text for morph analysis: {sample_text_morph}\")\n",
    "\n",
    "tokens_morph = tokenize_text(sample_text_morph)\n",
    "print(f\"Tokens: {tokens_morph}\")\n",
    "\n",
    "morphological_data_full = get_morphological_analysis(tokens_morph)\n",
    "print(\"\\nMorphological analysis of tokenized text:\")\n",
    "for item in morphological_data_full:\n",
    "    print(f\"Token: {item['token']}, Morphology: {item['tag']}\")\n",
    "\n",
    "print(\"\\n---\")\n",
    "print(\"\\nExample with lemmatized tokens (to see how tags might change or simplify):\")\n",
    "lemmatized_tokens_morph = lemmatize_tokens(tokens_morph) # Lemmatize first\n",
    "print(f\"Lemmatized Tokens: {lemmatized_tokens_morph}\")\n",
    "morphological_data_lemmatized = get_morphological_analysis(lemmatized_tokens_morph)\n",
    "for item in morphological_data_lemmatized:\n",
    "    print(f\"Token: {item['token']}, Morphology: {item['tag']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for statistical analysis\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# --- Term Frequency (TF) Calculation ---\n",
    "def calculate_tf(tokens):\n",
    "    \"\"\"Calculates Term Frequency for a list of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): A list of processed tokens (preferably lemmatized and without stop words).\n",
    "        \n",
    "    Returns:\n",
    "        nltk.probability.FreqDist: A FreqDist object containing token frequencies.\n",
    "    \"\"\"\n",
    "    fdist = FreqDist(tokens)\n",
    "    return fdist\n",
    "\n",
    "# --- Display Most Frequent Terms ---\n",
    "def display_most_frequent(fdist, top_n=10):\n",
    "    \"\"\"Displays the most frequent terms from a FreqDist object.\n",
    "    \n",
    "    Args:\n",
    "        fdist (nltk.probability.FreqDist): A FreqDist object or a dictionary of token frequencies.\n",
    "        top_n (int): The number of most frequent terms to display.\n",
    "    \"\"\"\n",
    "    print(f\"Top {top_n} most frequent terms:\")\n",
    "    for word, count in fdist.most_common(top_n):\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"Statistical analysis functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage of Statistical Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage of Statistical Analysis Functions\n",
    "sample_text_stats = \"Это пример текста, который является примером и используется для демонстрации статистического анализа текста. Анализ текста важен для понимания текста.\"\n",
    "print(f\"Original text for statistical analysis: {sample_text_stats}\")\n",
    "\n",
    "# Preprocess the text (Tokenize, Lemmatize, Remove Stopwords)\n",
    "tokens_stats = tokenize_text(sample_text_stats)\n",
    "print(f\"\\nTokens: {tokens_stats}\")\n",
    "\n",
    "lemmatized_stats = lemmatize_tokens(tokens_stats)\n",
    "print(f\"\\nLemmatized Tokens: {lemmatized_stats}\")\n",
    "\n",
    "no_stopwords_stats = remove_stopwords(lemmatized_stats)\n",
    "print(f\"\\nProcessed (no stopwords) tokens for stats: {no_stopwords_stats}\")\n",
    "\n",
    "# Calculate TF\n",
    "term_frequencies = calculate_tf(no_stopwords_stats)\n",
    "print(f\"\\nTerm Frequencies (FreqDist): {term_frequencies}\") # Shows the FreqDist object\n",
    "# To see it as a dictionary:\n",
    "# print(f\"Term Frequencies (dict): {dict(term_frequencies)}\")\n",
    "\n",
    "# Display most frequent terms\n",
    "display_most_frequent(term_frequencies, top_n=5)\n",
    "\n",
    "print(\"\\n---\")\n",
    "print(\"Note: For more advanced term importance, especially across multiple documents, TF-IDF (Term Frequency-Inverse Document Frequency) would be a next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Analysis Functions (spaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy and numpy (for potential vector operations if needed)\n",
    "import spacy\n",
    "import numpy as np # Usually not directly needed for basic similarity, but good to have for vector math\n",
    "\n",
    "# Load the spaCy model for Russian.\n",
    "# Note: 'ru_core_news_sm' is small and has basic vectors.\n",
    "# For better quality word vectors, consider using larger models:\n",
    "# !python -m spacy download ru_core_news_md\n",
    "# nlp_spacy = spacy.load('ru_core_news_md')\n",
    "# OR\n",
    "# !python -m spacy download ru_core_news_lg\n",
    "# nlp_spacy = spacy.load('ru_core_news_lg')\n",
    "try:\n",
    "    nlp_spacy = spacy.load('ru_core_news_sm')\n",
    "except OSError:\n",
    "    print('spaCy Russian model not found. Please run in the first cell:\\n!python -m spacy download ru_core_news_sm')\n",
    "    # Create a dummy nlp_spacy object to avoid errors in function definitions if model isn't downloaded yet\n",
    "    class DummySpacyNLP:\n",
    "        def __call__(self, text):\n",
    "            class DummyDoc:\n",
    "                def __init__(self, text):\n",
    "                    self.text = text\n",
    "                    self.vector = np.zeros((1,)) # Dummy vector\n",
    "                    self.has_vector = False\n",
    "                def similarity(self, other_doc):\n",
    "                    return 0.0\n",
    "            return DummyDoc(text)\n",
    "    nlp_spacy = DummySpacyNLP()\n",
    "\n",
    "# --- Get Word Vector ---\n",
    "def get_word_vector(token_text, nlp_model):\n",
    "    \"\"\"Returns the word vector for a token using a spaCy model.\"\"\"\n",
    "    return nlp_model(token_text).vector\n",
    "\n",
    "# --- Calculate Similarity Between Two Tokens ---\n",
    "def calculate_similarity_between_tokens(token1_text, token2_text, nlp_model):\n",
    "    \"\"\"Calculates cosine similarity between two tokens using a spaCy model.\"\"\"\n",
    "    token1 = nlp_model(str(token1_text).lower()) # Process as string and lowercase\n",
    "    token2 = nlp_model(str(token2_text).lower())\n",
    "    if token1.has_vector and token2.has_vector:\n",
    "        return token1.similarity(token2)\n",
    "    else:\n",
    "        # print(f\"Warning: Vectors not found for '{token1.text}' or '{token2.text}'. Similarity is 0.\")\n",
    "        return 0.0\n",
    "\n",
    "# --- Find Most Similar Tokens from a List ---\n",
    "def find_most_similar_from_list(target_token_text, candidate_tokens_list, nlp_model, top_n=3):\n",
    "    \"\"\"Finds the most similar tokens to a target token from a given list.\"\"\"\n",
    "    target_token = nlp_model(str(target_token_text).lower())\n",
    "    if not target_token.has_vector:\n",
    "        print(f\"Warning: No vector for target token '{target_token.text}' in the model.\")\n",
    "        return []\n",
    "    \n",
    "    similarities = []\n",
    "    for candidate_text in candidate_tokens_list:\n",
    "        candidate_token = nlp_model(str(candidate_text).lower())\n",
    "        if candidate_token.has_vector and candidate_token.text != target_token.text: # Ensure not comparing to itself\n",
    "            similarity_score = target_token.similarity(candidate_token)\n",
    "            similarities.append((candidate_token.text, similarity_score))\n",
    "            \n",
    "    # Sort by similarity score in descending order\n",
    "    similarities.sort(key=lambda item: item[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "print(\"Semantic analysis functions (spaCy) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage of Semantic Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage of Semantic Analysis Functions\n",
    "\n",
    "# Ensure nlp_spacy is loaded (it should be from the cell above)\n",
    "if isinstance(nlp_spacy, spacy.lang.ru.Russian):\n",
    "    print(f\"spaCy model '{nlp_spacy.meta['name']}' loaded successfully for semantic analysis.\\n\")\n",
    "else:\n",
    "    print(\"spaCy model was not loaded successfully. Semantic analysis examples might not work.\\n\")\n",
    "\n",
    "sample_text_semantic = \"Компьютер это мощное устройство для вычислений и обработки данных. Телефон также является устройством связи.\"\n",
    "print(f\"Sample text for semantic analysis: {sample_text_semantic}\")\n",
    "\n",
    "# Tokenize the text (using existing function for consistency, though spaCy can tokenize directly)\n",
    "raw_tokens_semantic = tokenize_text(sample_text_semantic) \n",
    "print(f\"\\nRaw tokens from NLTK tokenizer: {raw_tokens_semantic}\")\n",
    "\n",
    "# For spaCy similarity, it's often good to work with lemmas or lowercase individual words.\n",
    "# We will use lowercase versions of the NLTK-tokenized words for this example.\n",
    "\n",
    "# 1. Demonstrate getting a word vector\n",
    "example_token_for_vector = \"компьютер\" # Using lowercase directly\n",
    "if nlp_spacy(example_token_for_vector).has_vector:\n",
    "    vector = get_word_vector(example_token_for_vector, nlp_spacy)\n",
    "    print(f\"\\nVector for '{example_token_for_vector}' (first 5 elements): {vector[:5]}...\")\n",
    "else:\n",
    "    print(f\"\\nNo vector found for '{example_token_for_vector}' in the current spaCy model.\")\n",
    "\n",
    "# 2. Demonstrate similarity calculation between two tokens\n",
    "token_a_text = \"компьютер\"\n",
    "token_b_text = \"устройство\"\n",
    "similarity_score = calculate_similarity_between_tokens(token_a_text, token_b_text, nlp_spacy)\n",
    "print(f\"\\nSimilarity between '{token_a_text}' and '{token_b_text}': {similarity_score:.4f}\")\n",
    "\n",
    "token_c_text = \"телефон\"\n",
    "similarity_score_2 = calculate_similarity_between_tokens(token_a_text, token_c_text, nlp_spacy)\n",
    "print(f\"Similarity between '{token_a_text}' and '{token_c_text}': {similarity_score_2:.4f}\")\n",
    "\n",
    "token_d_text = \"книга\"\n",
    "similarity_score_3 = calculate_similarity_between_tokens(token_a_text, token_d_text, nlp_spacy)\n",
    "print(f\"Similarity between '{token_a_text}' and '{token_d_text}': {similarity_score_3:.4f}\")\n",
    "\n",
    "# 3. Demonstrate finding most similar tokens from a predefined list\n",
    "target_word_semantic = \"обработка\"\n",
    "# Use unique, lemmatized, and stopword-removed tokens from the sample text as candidates, or a custom list\n",
    "lemmatized_semantic = lemmatize_tokens(raw_tokens_semantic)\n",
    "processed_tokens_semantic = remove_stopwords(lemmatized_semantic) # These are already lowercase\n",
    "candidate_list_semantic = list(set(processed_tokens_semantic)) # Unique tokens from the sample text\n",
    "print(f\"\\nFinding words similar to '{target_word_semantic}' from candidates: {candidate_list_semantic}\")\n",
    "\n",
    "similar_tokens_list = find_most_similar_from_list(target_word_semantic, candidate_list_semantic, nlp_spacy, top_n=3)\n",
    "\n",
    "if similar_tokens_list:\n",
    "    print(f\"Most similar tokens to '{target_word_semantic}':\")\n",
    "    for token, score in similar_tokens_list:\n",
    "        print(f\"- {token}: {score:.4f}\")\n",
    "else:\n",
    "    print(f\"Could not find similar tokens for '{target_word_semantic}' from the list or it has no vector.\")\n",
    "\n",
    "print(\"\\nNote: Similarity scores depend heavily on the quality of word vectors in the loaded spaCy model.\")\n",
    "print(\"Using 'ru_core_news_md' or 'ru_core_news_lg' would likely provide more nuanced results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network for Term Extraction (BiLSTM Model)\n",
    "\n",
    "This section outlines the basic structure for a Bidirectional LSTM model using TensorFlow/Keras for sequence tagging, which is a common approach for term extraction. This is a placeholder and requires real annotated data for training and meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports and Setup for Neural Network ---\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, TimeDistributed, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# --- Constants and Mappings (Placeholders) ---\n",
    "MAX_SEQ_LEN = 50      # Max length of a sequence (sentence)\n",
    "EMBEDDING_DIM = 100   # Dimension of word embeddings\n",
    "LSTM_UNITS = 64       # Number of units in LSTM layer\n",
    "VOCAB_SIZE = 1000     # Placeholder: actual size depends on your corpus, e.g., len(word_to_idx)\n",
    "NUM_TAGS = 3          # Example: 0 for O (Outside), 1 for B-TERM (Begin), 2 for I-TERM (Inside)\n",
    "\n",
    "# Dummy mappings (replace with actual mappings from your data)\n",
    "word_to_idx_nn = {'<pad>': 0, '<UNK>': 1, 'слово': 2, 'термин': 3, 'анализ': 4, 'данные': 5, 'текст': 6} # Extended for dummy data + UNK\n",
    "tag_to_idx_nn = {'O': 0, 'B-TERM': 1, 'I-TERM': 2}\n",
    "idx_to_tag_nn = {v: k for k, v in tag_to_idx_nn.items()}\n",
    "\n",
    "# Update VOCAB_SIZE based on the dummy mapping for this example\n",
    "VOCAB_SIZE = len(word_to_idx_nn)\n",
    "\n",
    "# --- 1. Data Preparation Placeholder Function ---\n",
    "def prepare_nn_data(texts_tokens_list, tags_list, word_to_idx_map, tag_to_idx_map, max_seq_len, num_tags_val):\n",
    "    \"\"\"Generates dummy sequences of word indices and tag indices.\n",
    "    In a real scenario:\n",
    "    1. Convert lists of tokens (texts_tokens_list) to sequences of integers using word_to_idx_map.\n",
    "    2. Convert lists of tags (tags_list) to sequences of integers using tag_to_idx_map.\n",
    "    3. Pad both token and tag sequences to max_seq_len.\n",
    "    4. One-hot encode the tag sequences if using 'categorical_crossentropy' loss.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Dummy Data for NN --- \")\n",
    "    num_samples = 5 # Number of dummy samples to generate\n",
    "    min_len, max_len = 5, max_seq_len - 5 # Min/max length for random sequences before padding\n",
    "\n",
    "    # Generate dummy word index sequences\n",
    "    # Each sample will have a random length between min_len and max_len\n",
    "    X_dummy_sequences = []\n",
    "    for _ in range(num_samples):\n",
    "        seq_len = np.random.randint(min_len, max_len + 1)\n",
    "        # Ensure values are within the valid range of word_to_idx_map keys (excluding <pad> and <UNK> for generation)\n",
    "        sequence = np.random.randint(2, len(word_to_idx_map), size=seq_len) # Start from index 2 to avoid <pad> and <UNK>\n",
    "        X_dummy_sequences.append(sequence)\n",
    "    \n",
    "    X_padded = pad_sequences(X_dummy_sequences, maxlen=max_seq_len, padding='post', value=word_to_idx_map.get('<pad>', 0))\n",
    "    \n",
    "    # Generate dummy tag index sequences (corresponding to the unpadded length of X)\n",
    "    y_dummy_indices_list = []\n",
    "    for seq in X_dummy_sequences: # Use the length of the original unpadded sequences for tags\n",
    "        tag_sequence = np.random.randint(0, num_tags_val, size=len(seq))\n",
    "        y_dummy_indices_list.append(tag_sequence)\n",
    "        \n",
    "    y_padded_indices = pad_sequences(y_dummy_indices_list, maxlen=max_seq_len, padding='post', value=tag_to_idx_map.get('O', 0)) # Pad with 'O' tag\n",
    "    y_padded_one_hot = to_categorical(y_padded_indices, num_classes=num_tags_val)\n",
    "    \n",
    "    print(f\"Generated {num_samples} dummy samples.\")\n",
    "    print(f\"X_padded shape: {X_padded.shape} (Samples, Max Sequence Length)\")\n",
    "    print(f\"y_padded_one_hot shape: {y_padded_one_hot.shape} (Samples, Max Sequence Length, Num Tags)\")\n",
    "    print(\"Note: This is DUMMY data. Real data requires careful preprocessing and annotation.\")\n",
    "    return X_padded, y_padded_one_hot\n",
    "\n",
    "# --- 2. Build BiLSTM Model Function ---\n",
    "def build_bilstm_model(max_seq_len_val, vocab_size_val, embedding_dim_val, lstm_units_val, num_tags_val):\n",
    "    \"\"\"Builds and compiles a BiLSTM model for sequence tagging.\"\"\"\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(max_seq_len_val,))\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = Embedding(input_dim=vocab_size_val, \n",
    "                                output_dim=embedding_dim_val, \n",
    "                                input_length=max_seq_len_val,\n",
    "                                mask_zero=True)(input_layer) # mask_zero=True is useful if padding with 0\n",
    "    \n",
    "    # BiLSTM layer\n",
    "    bilstm_layer = Bidirectional(LSTM(units=lstm_units_val, \n",
    "                                      return_sequences=True, \n",
    "                                      recurrent_dropout=0.1))(embedding_layer)\n",
    "    \n",
    "    # TimeDistributed Dense output layer\n",
    "    # Applies a Dense layer to each time step of the BiLSTM output\n",
    "    output_layer = TimeDistributed(Dense(num_tags_val, activation=\"softmax\"))(bilstm_layer)\n",
    "    \n",
    "    # Create the Keras Model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=\"adam\", \n",
    "                  loss=\"categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# --- 3. Training Placeholder Function ---\n",
    "def train_nn_model(model, X_train, y_train, epochs_val=1, batch_size_val=2): # Reduced defaults for dummy run\n",
    "    \"\"\"Trains the model. For this placeholder, it runs for a few epochs on dummy data.\"\"\"\n",
    "    print(\"\\n--- Training Neural Network Model (Placeholder) ---\")\n",
    "    if X_train is None or y_train is None:\n",
    "        print(\"No training data provided. Skipping actual training call.\")\n",
    "        return model\n",
    "        \n",
    "    print(f\"Starting 'training' for {epochs_val} epoch(s) with batch size {batch_size_val}...\")\n",
    "    # In a real scenario, use more epochs, proper validation split, callbacks, etc.\n",
    "    model.fit(X_train, y_train, epochs=epochs_val, batch_size=batch_size_val, verbose=1)\n",
    "    print(\"Placeholder 'training' complete. With real data, this would involve more extensive training.\")\n",
    "    return model\n",
    "\n",
    "# --- 4. Prediction Function ---\n",
    "def predict_terms_nn(model, text_sequence_padded_idx, idx_to_tag_map):\n",
    "    \"\"\"Predicts tag labels for a given preprocessed text sequence.\"\"\"\n",
    "    if model is None:\n",
    "        # Try to build and train a dummy model if not available (e.g., if pipeline run without running NN example cell)\n",
    "        print(\"NN model not found. Building and 'training' a dummy one for pipeline demonstration.\")\n",
    "        temp_X, temp_y = prepare_nn_data(None, None, word_to_idx_nn, tag_to_idx_nn, MAX_SEQ_LEN, NUM_TAGS)\n",
    "        temp_model = build_bilstm_model(MAX_SEQ_LEN, VOCAB_SIZE, EMBEDDING_DIM, LSTM_UNITS, NUM_TAGS)\n",
    "        temp_model = train_nn_model(temp_model, temp_X, temp_y, epochs_val=1, batch_size_val=1)\n",
    "        # Make the temporary model global for subsequent calls in the same pipeline run if needed\n",
    "        # Or better, ensure nn_model is always passed or available in the scope calling this.\n",
    "        # For this function, we'll use the locally trained temp_model for prediction here.\n",
    "        # This is not ideal for performance but makes pipeline more robust to cell execution order for demo.\n",
    "        if temp_model is None:\n",
    "            return [\"Error: Dummy model could not be built for prediction.\"] * len(text_sequence_padded_idx[0])\n",
    "        prediction_model = temp_model\n",
    "    else:\n",
    "        prediction_model = model\n",
    "        \n",
    "    # Model expects a batch, so even for a single sequence, it should be shaped (1, max_seq_len)\n",
    "    if len(text_sequence_padded_idx.shape) == 1: # If it's a flat array\n",
    "        text_sequence_padded_idx = np.expand_dims(text_sequence_padded_idx, axis=0)\n",
    "        \n",
    "    # Get tag probabilities from the model\n",
    "    tag_probabilities = prediction_model.predict(text_sequence_padded_idx, verbose=0) # verbose=0 to reduce output in pipeline\n",
    "    \n",
    "    # Convert probabilities to tag indices (select the tag with the highest probability at each step)\n",
    "    predicted_tag_indices = np.argmax(tag_probabilities, axis=-1)\n",
    "    \n",
    "    # Convert indices to tag labels\n",
    "    predicted_labels = [idx_to_tag_map.get(idx, 'UNK') for idx in predicted_tag_indices[0]] # [0] because we predict one sequence at a time\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "print(\"Neural Network (BiLSTM) functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage of Neural Network Model (BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage / Demonstration of BiLSTM ---\n",
    "print(\"\\n--- Starting Neural Network (BiLSTM) Demonstration ---\")\n",
    "\n",
    "# 1. Generate Dummy Data\n",
    "# In a real scenario, 'texts_tokens_list' and 'tags_list' would come from your annotated dataset.\n",
    "X_train_dummy, y_train_dummy_one_hot = prepare_nn_data(\n",
    "    texts_tokens_list=None, # Placeholder for actual tokenized sentences\n",
    "    tags_list=None,         # Placeholder for actual tag sequences\n",
    "    word_to_idx_map=word_to_idx_nn,\n",
    "    tag_to_idx_map=tag_to_idx_nn,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    num_tags_val=NUM_TAGS\n",
    ")\n",
    "\n",
    "# 2. Build the Model\n",
    "print(\"\\n--- Building BiLSTM Model ---\")\n",
    "nn_model = build_bilstm_model(\n",
    "    max_seq_len_val=MAX_SEQ_LEN, \n",
    "    vocab_size_val=VOCAB_SIZE, # Make sure VOCAB_SIZE is updated if word_to_idx_nn changes\n",
    "    embedding_dim_val=EMBEDDING_DIM, \n",
    "    lstm_units_val=LSTM_UNITS, \n",
    "    num_tags_val=NUM_TAGS\n",
    ")\n",
    "\n",
    "if nn_model:\n",
    "    nn_model.summary() # Print model architecture\n",
    "else:\n",
    "    print(\"Model building returned None.\")\n",
    "\n",
    "# 3. \"Train\" the Model (on dummy data for demonstration)\n",
    "if nn_model:\n",
    "    nn_model = train_nn_model(nn_model, X_train_dummy, y_train_dummy_one_hot, epochs_val=1, batch_size_val=2)\n",
    "else:\n",
    "    print(\"Skipping training as model was not built.\")\n",
    "\n",
    "# 4. Predict on a Sample Test Sequence (dummy)\n",
    "print(\"\\n--- Predicting with BiLSTM Model (Dummy Example) ---\")\n",
    "if nn_model and X_train_dummy.shape[0] > 0:\n",
    "    # Take the first dummy sequence from the generated training data as a sample for prediction\n",
    "    sample_test_sequence_idx = X_train_dummy[0:1] # Shape (1, MAX_SEQ_LEN)\n",
    "    original_input_tokens_indices = sample_test_sequence_idx[0]\n",
    "\n",
    "    predicted_tag_labels = predict_terms_nn(nn_model, sample_test_sequence_idx, idx_to_tag_nn)\n",
    "    \n",
    "    print(f\"\\nSample Input (dummy word indices, first 15): {original_input_tokens_indices[:15]}...\")\n",
    "    # For clarity, let's map some input indices back to words (if they exist in our tiny dummy map)\n",
    "    idx_to_word_nn_rev = {v: k for k,v in word_to_idx_nn.items()} # Corrected variable name for reverse map\n",
    "    sample_input_words = [idx_to_word_nn_rev.get(idx, '?') for idx in original_input_tokens_indices[:15]]\n",
    "    print(f\"Sample Input (dummy words, first 15):       {sample_input_words}...\")\n",
    "    print(f\"Predicted Tags (first 15):                {predicted_tag_labels[:15]}...\")\n",
    "    \n",
    "    # Show actual (dummy) Y for comparison for the first sample\n",
    "    actual_tags_one_hot = y_train_dummy_one_hot[0]\n",
    "    actual_tags_indices = np.argmax(actual_tags_one_hot, axis=-1)\n",
    "    actual_labels = [idx_to_tag_nn.get(idx, 'UNK') for idx in actual_tags_indices[:15]]\n",
    "    print(f\"Actual Tags (dummy, first 15):            {actual_labels}...\")\n",
    "else:\n",
    "    print(\"Skipping prediction as model or dummy data is not available.\")\n",
    "\n",
    "print(\"\\n--- Neural Network (BiLSTM) Demonstration Complete ---\")\n",
    "print(\"Reminder: This setup uses DUMMY data and MAPPINGS. For real term extraction, use annotated text data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Glossary Management ---\n",
    "glossary = [] # Initialize the global glossary list\n",
    "\n",
    "# --- Add Term to Glossary ---\n",
    "def add_to_glossary(term_text, source_method, details_dict=None):\n",
    "    \"\"\"Adds a term to the global glossary, checking for case-insensitive duplicates.\"\"\"\n",
    "    global glossary\n",
    "    term_text_lower = term_text.lower() # For case-insensitive check\n",
    "    \n",
    "    # Check for existing term (case-insensitive)\n",
    "    for entry in glossary:\n",
    "        if entry['term'].lower() == term_text_lower:\n",
    "            print(f\"Term '{term_text}' (as '{entry['term']}') already in glossary. (Original source: {entry['source']}). Not adding again.\")\n",
    "            # Optional: Could update details or add new source here if desired.\n",
    "            # For example, if entry['source'] != source_method, append to a list of sources.\n",
    "            return\n",
    "\n",
    "    new_entry = {\n",
    "        'term': term_text, \n",
    "        'source': source_method, \n",
    "        'details': details_dict if details_dict is not None else {}\n",
    "    }\n",
    "    glossary.append(new_entry)\n",
    "    print(f\"Term '{term_text}' added to glossary from {source_method}.\")\n",
    "\n",
    "# --- Display Glossary ---\n",
    "def display_glossary():\n",
    "    \"\"\"Displays all terms in the global glossary.\"\"\"\n",
    "    global glossary\n",
    "    print(\"\\n--- Glossary ---\")\n",
    "    if not glossary:\n",
    "        print(\"The glossary is currently empty.\")\n",
    "        return\n",
    "    \n",
    "    for i, entry in enumerate(glossary):\n",
    "        term_info = f\"{i+1}. Term: {entry['term']}\\n\"\n",
    "        term_info += f\"   Source: {entry['source']}\\n\"\n",
    "        if entry['details']:\n",
    "            term_info += f\"   Details: {entry['details']}\"\n",
    "        else:\n",
    "            term_info += f\"   Details: N/A\"\n",
    "        print(term_info)\n",
    "\n",
    "print(\"Glossary management functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage of Glossary Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Testing Glossary Management ---\n",
    "print(\"\\n--- Testing Glossary Management ---\")\n",
    "\n",
    "# Re-initialize glossary for clean testing if this cell is run multiple times\n",
    "glossary = [] \n",
    "print(\"Glossary reset for this test run.\")\n",
    "\n",
    "add_to_glossary(\"машинное обучение\", \"manual\", {'definition': 'Раздел ИИ, изучающий методы построения алгоритмов, способных обучаться.'})\n",
    "add_to_glossary(\"анализ данных\", \"statistical\", {'frequency': 15, 'document_id': 'doc_A01'})\n",
    "add_to_glossary(\"нейронная сеть\", \"NN_placeholder\", {'confidence': 0.88, 'layers': 3})\n",
    "add_to_glossary(\"Естественный язык\", \"semantic_similarity\", {'related_to': 'обработка текста', 'score': 0.92})\n",
    "\n",
    "# Attempt to add a duplicate term (case-insensitive)\n",
    "add_to_glossary(\"Анализ Данных\", \"manual\") # Should be detected as duplicate\n",
    "add_to_glossary(\"нейронная Сеть\", \"other_method\") # Another duplicate test\n",
    "\n",
    "display_glossary()\n",
    "\n",
    "print(\"\\n--- Adding terms from a hypothetical list ---\")\n",
    "hypothetical_terms = [\n",
    "    (\"обработка текста\", \"statistical\", {\"score\": 0.75, \"co_occurrence_with\": \"алгоритм\"}),\n",
    "    (\"ключевое слово\", \"semantic\", {\"related_to\": \"термин\", \"similarity\": 0.85}),\n",
    "    (\"Машинное Обучение\", \"review\", {\"notes\": \"Needs verification by expert\"}) # Duplicate check\n",
    "]\n",
    "\n",
    "for term, source, details in hypothetical_terms:\n",
    "    add_to_glossary(term, source, details)\n",
    "\n",
    "display_glossary()\n",
    "\n",
    "# Test adding a term with no details\n",
    "add_to_glossary(\"Простой Термин\", \"manual_input\")\n",
    "display_glossary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization Imports---\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "# NLTK FreqDist is used by plot_term_frequencies, ensure it's available\n",
    "# It's imported in the 'Statistical Analysis Functions' section, so it should be in scope\n",
    "# from nltk.probability import FreqDist \n",
    "\n",
    "# --- Generate Word Cloud ---\n",
    "def generate_word_cloud(tokens_list, font_path='/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', width=800, height=400, background_color='white'):\n",
    "    \"\"\"Generates and displays a word cloud from a list of tokens.\"\"\"\n",
    "    text = \" \".join(tokens_list)\n",
    "    if not text.strip(): # Check if the text is empty or only whitespace\n",
    "        print(\"Cannot generate word cloud from empty text.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        wordcloud_obj = WordCloud(width=width, height=height, \n",
    "                                  background_color=background_color, \n",
    "                                  font_path=font_path,\n",
    "                                  collocations=False).generate(text) # Added collocations=False for individual words\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud_obj, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    except RuntimeError as e: # Handle font not found or other WordCloud errors\n",
    "        print(f\"Error generating word cloud (likely font issue or empty text): {e}\")\n",
    "        print(\"Please ensure a Cyrillic-supporting font is available at the specified font_path.\")\n",
    "        print(\"In Colab, DejaVuSans.ttf is usually available. If not, upload a .ttf font and update font_path, or install one system-wide.\")\n",
    "    except Exception as e: # Catch any other unexpected errors\n",
    "        print(f\"An unexpected error occurred during word cloud generation: {e}\")\n",
    "\n",
    "# --- Plot Term Frequencies ---\n",
    "def plot_term_frequencies(fdist, top_n=10, title='Top N Term Frequencies'):\n",
    "    \"\"\"Plots a bar chart of the most frequent terms from a FreqDist object.\"\"\"\n",
    "    if not isinstance(fdist, FreqDist) and not isinstance(fdist, dict):\n",
    "        print(\"Invalid input: fdist must be an NLTK FreqDist object or a dictionary.\")\n",
    "        return\n",
    "    if not fdist:\n",
    "        print(\"Frequency distribution is empty. Cannot plot.\")\n",
    "        return\n",
    "        \n",
    "    # Get most common items. If fdist is dict, convert to FreqDist for most_common\n",
    "    if isinstance(fdist, dict):\n",
    "        fdist_converted = FreqDist(fdist)\n",
    "    else:\n",
    "        fdist_converted = fdist\n",
    "        \n",
    "    common_words = fdist_converted.most_common(top_n)\n",
    "    \n",
    "    if not common_words:\n",
    "        print(\"No common words to plot (possibly after filtering or for low top_n).\" )\n",
    "        return\n",
    "        \n",
    "    terms, frequencies = zip(*common_words)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(terms, frequencies, color='skyblue')\n",
    "    plt.xlabel('Terms', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization functions (generate_word_cloud, plot_term_frequencies) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage of Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Testing Visualization Functions ---\n",
    "print(\"\\n--- Testing Visualization Functions ---\")\n",
    "\n",
    "sample_text_viz = \"Это пример текста для визуализации частоты слов и облака слов. \" \\\n",
    "                  \"Повторяем слова: текст, слова, пример. Ещё немного текста про слова и анализ. \" \\\n",
    "                  \"Визуализация это важный этап анализа данных. Анализ, анализ, анализ!\"\n",
    "\n",
    "print(f\"Original text for visualization: {sample_text_viz}\")\n",
    "\n",
    "# Preprocess the text (using existing functions)\n",
    "tokens_viz = tokenize_text(sample_text_viz)\n",
    "lemmatized_viz = lemmatize_tokens(tokens_viz)\n",
    "no_stopwords_viz = remove_stopwords(lemmatized_viz)\n",
    "\n",
    "print(f\"\\nProcessed tokens for visualization: {no_stopwords_viz}\")\n",
    "\n",
    "if no_stopwords_viz:\n",
    "    # Generate Word Cloud\n",
    "    print(\"\\nGenerating Word Cloud...\")\n",
    "    generate_word_cloud(no_stopwords_viz) # Using default font_path\n",
    "    # generate_word_cloud(no_stopwords_viz, font_path='non_existent_font.ttf') # Test font error handling\n",
    "\n",
    "    # Plot Term Frequencies\n",
    "    print(\"\\nPlotting Term Frequencies...\")\n",
    "    fdist_viz = calculate_tf(no_stopwords_viz) # Assumes calculate_tf is defined\n",
    "    plot_term_frequencies(fdist_viz, top_n=7, title=\"Top 7 Frequent Terms in Sample Text\")\n",
    "    \n",
    "    # Test with an empty list of tokens for word cloud\n",
    "    print(\"\\nTesting word cloud with empty token list...\")\n",
    "    generate_word_cloud([])\n",
    "    \n",
    "    # Test with empty FreqDist for bar plot\n",
    "    print(\"\\nTesting frequency plot with empty FreqDist...\")\n",
    "    plot_term_frequencies(FreqDist())\n",
    "else:\n",
    "    print(\"No tokens to visualize after preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Term Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- End-to-End Term Extraction Pipeline Function ---\n",
    "def run_term_extraction_pipeline(raw_russian_text):\n",
    "    \"\"\"Integrates and demonstrates the full term extraction workflow.\"\"\"\n",
    "    global glossary # Using the global glossary list, re-initialize for each run\n",
    "    global nn_model # Assuming nn_model is globally available or built by its example cell\n",
    "    # Other globals assumed to be available: morph, nlp_spacy, word_to_idx_nn, idx_to_tag_nn, MAX_SEQ_LEN, NUM_TAGS\n",
    "\n",
    "    print(\"--- Starting Term Extraction Pipeline ---\")\n",
    "    print(f\"Original Text:\\n{raw_russian_text}\\n\")\n",
    "\n",
    "    # 1. Initial Setup - Reset Glossary for this run\n",
    "    glossary = []\n",
    "    print(\"Glossary reset for this pipeline run.\")\n",
    "\n",
    "    # 2. Preprocessing\n",
    "    print(\"\\n--- 1. Preprocessing Stage ---\")\n",
    "    tokens = tokenize_text(raw_russian_text)\n",
    "    print(f\"Tokens (first 10): {tokens[:10]}...\")\n",
    "    lemmatized_tokens = lemmatize_tokens(tokens)\n",
    "    print(f\"Lemmatized Tokens (first 10): {lemmatized_tokens[:10]}...\")\n",
    "    filtered_tokens = remove_stopwords(lemmatized_tokens)\n",
    "    print(f\"Filtered Tokens (first 10): {filtered_tokens[:10]}...\")\n",
    "\n",
    "    if not filtered_tokens:\n",
    "        print(\"No tokens remaining after preprocessing. Pipeline cannot continue meaningfully.\")\n",
    "        return\n",
    "\n",
    "    # 3. Morphological Analysis\n",
    "    print(\"\\n--- 2. Morphological Analysis (First 5 Filtered Tokens) ---\")\n",
    "    morph_analysis_results = get_morphological_analysis(filtered_tokens[:5])\n",
    "    for res in morph_analysis_results:\n",
    "        print(f\"Token: {res['token']}, Tag: {res['tag']}\")\n",
    "\n",
    "    # 4. Statistical Analysis\n",
    "    print(\"\\n--- 3. Statistical Analysis ---\")\n",
    "    fdist = calculate_tf(filtered_tokens)\n",
    "    display_most_frequent(fdist, top_n=5)\n",
    "    print(\"Adding top 3 frequent terms to glossary:\")\n",
    "    for term, freq in fdist.most_common(3):\n",
    "        add_to_glossary(term, \"statistical (TF)\", {'frequency': freq})\n",
    "\n",
    "    # 5. Semantic Analysis (Demonstration)\n",
    "    print(\"\\n--- 4. Semantic Analysis (Demonstration) ---\")\n",
    "    if len(filtered_tokens) >= 2:\n",
    "        token1_sem = filtered_tokens[0]\n",
    "        token2_sem = filtered_tokens[1]\n",
    "        # Pick a common related word and an unrelated one for demonstration\n",
    "        related_word_example = \"данные\" # (data)\n",
    "        unrelated_word_example = \"кошка\" # (cat)\n",
    "        \n",
    "        print(f\"Calculating similarity for '{token1_sem}':\")\n",
    "        sim1 = calculate_similarity_between_tokens(token1_sem, related_word_example, nlp_spacy)\n",
    "        print(f\"  Similarity between '{token1_sem}' and '{related_word_example}': {sim1:.4f}\")\n",
    "        sim2 = calculate_similarity_between_tokens(token1_sem, unrelated_word_example, nlp_spacy)\n",
    "        print(f\"  Similarity between '{token1_sem}' and '{unrelated_word_example}': {sim2:.4f}\")\n",
    "        \n",
    "        # Find similar from a small candidate list\n",
    "        candidate_list_sem = [t for t in filtered_tokens[1:6] if t != token1_sem] # a few other unique tokens from text\n",
    "        if candidate_list_sem:\n",
    "            print(f\"Finding terms similar to '{token1_sem}' from candidates: {candidate_list_sem}\")\n",
    "            similar_list = find_most_similar_from_list(token1_sem, candidate_list_sem, nlp_spacy, top_n=2)\n",
    "            if similar_list:\n",
    "                for term, score in similar_list:\n",
    "                    print(f\"  - {term}: {score:.4f}\")\n",
    "            else:\n",
    "                print(\"  No similar tokens found in the candidate list or target has no vector.\")\n",
    "        else:\n",
    "            print(\"  Not enough unique candidate tokens for 'find_most_similar_from_list' demo.\")\n",
    "    else:\n",
    "        print(\"Not enough filtered tokens for semantic analysis demonstration.\")\n",
    "\n",
    "    # 6. Neural Network Term Extraction (Simulated)\n",
    "    print(\"\\n--- 5. Neural Network Term Extraction (SIMULATED) ---\")\n",
    "    print(\"NOTE: The following NN results are from an UNTRAINED model using DUMMY mappings & data.\")\n",
    "    print(\"This is for demonstrating pipeline flow ONLY.\")\n",
    "    \n",
    "    # Convert filtered_tokens to sequences of indices\n",
    "    # Using word_to_idx_nn which now includes '<UNK>'\n",
    "    indexed_tokens = [word_to_idx_nn.get(token, word_to_idx_nn['<UNK>']) for token in filtered_tokens]\n",
    "    padded_sequence = pad_sequences([indexed_tokens], maxlen=MAX_SEQ_LEN, padding='post', value=word_to_idx_nn['<pad>'])\n",
    "    \n",
    "    # Ensure nn_model is available (it should be if the NN example cell was run)\n",
    "    global nn_model # Explicitly declare usage of global model for clarity\n",
    "    if 'nn_model' not in globals() or nn_model is None:\n",
    "        print(\"NN model is not available. Building and training a dummy one for this run...\")\n",
    "        # This part is to make the pipeline runnable even if NN example cell wasn't run\n",
    "        # It uses the global constants like MAX_SEQ_LEN, VOCAB_SIZE etc.\n",
    "        temp_X, temp_y = prepare_nn_data(None, None, word_to_idx_nn, tag_to_idx_nn, MAX_SEQ_LEN, NUM_TAGS)\n",
    "        nn_model = build_bilstm_model(MAX_SEQ_LEN, VOCAB_SIZE, EMBEDDING_DIM, LSTM_UNITS, NUM_TAGS)\n",
    "        nn_model = train_nn_model(nn_model, temp_X, temp_y, epochs_val=1, batch_size_val=1)\n",
    "        print(\"Dummy NN model prepared.\")\n",
    "        \n",
    "    predicted_nn_tags = predict_terms_nn(nn_model, padded_sequence, idx_to_tag_nn)\n",
    "    \n",
    "    print(\"Hypothetically extracted NN terms (B-TERM or I-TERM predictions):\")\n",
    "    nn_term_count = 0\n",
    "    for i, token_text in enumerate(filtered_tokens):\n",
    "        if i < len(predicted_nn_tags): # Ensure we don't go out of bounds if padding affected tag list length\n",
    "            tag = predicted_nn_tags[i]\n",
    "            if tag in ['B-TERM', 'I-TERM']:\n",
    "                print(f\"  - Term: '{token_text}', Predicted Tag: {tag}\")\n",
    "                if nn_term_count < 2: # Add first two hypothetical terms to glossary\n",
    "                    add_to_glossary(token_text, \"NN (simulated)\", {'predicted_tag': tag, 'confidence': 'N/A (dummy)'})\n",
    "                    nn_term_count += 1\n",
    "        else:\n",
    "            break # Stop if we run out of predicted tags\n",
    "    if nn_term_count == 0:\n",
    "        print(\"  No terms tagged as B-TERM or I-TERM by the simulated NN.\")\n",
    "\n",
    "    # 7. Glossary Display\n",
    "    print(\"\\n--- 6. Final Glossary ---\")\n",
    "    display_glossary()\n",
    "\n",
    "    # 8. Visualization\n",
    "    print(\"\\n--- 7. Visualizations ---\")\n",
    "    if filtered_tokens:\n",
    "        print(\"Generating Word Cloud for filtered tokens...\")\n",
    "        generate_word_cloud(filtered_tokens)\n",
    "        print(\"Plotting Term Frequencies for filtered tokens...\")\n",
    "        plot_term_frequencies(fdist, top_n=10, title=\"Top 10 Frequent Terms in Processed Text\")\n",
    "    else:\n",
    "        print(\"No filtered tokens to visualize.\")\n",
    "        \n",
    "    print(\"\\n--- Term Extraction Pipeline Complete ---\")\n",
    "\n",
    "print(\"End-to-end term extraction pipeline function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage of the Full Pipeline\n",
    "\n",
    "Before running the pipeline example, ensure that the necessary global variables and models from previous sections are initialized. This includes:\n",
    "- `morph` (Pymorphy2 MorphAnalyzer)\n",
    "- `nlp_spacy` (spaCy Russian model)\n",
    "- `word_to_idx_nn`, `tag_to_idx_nn`, `idx_to_tag_nn`, `MAX_SEQ_LEN`, `EMBEDDING_DIM`, `LSTM_UNITS`, `VOCAB_SIZE`, `NUM_TAGS` (for the NN part)\n",
    "- The `nn_model` itself (the dummy BiLSTM model built in the NN example section). The pipeline has a fallback to build it if not found, but it's better if the NN example cell is run first.\n",
    "\n",
    "The `glossary` list will be reset at the beginning of each pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example: Running the Full Term Extraction Pipeline ---\n",
    "print(\"\\n--- Example: Full Term Extraction Pipeline Run ---\")\n",
    "\n",
    "# Ensure global models/variables are available. \n",
    "# If any of these were not run in their respective cells, errors might occur or dummy models will be built.\n",
    "print(\"Checking for required global variables and models...\")\n",
    "required_globals = ['morph', 'nlp_spacy', 'word_to_idx_nn', 'idx_to_tag_nn', 'MAX_SEQ_LEN', \n",
    "                    'EMBEDDING_DIM', 'LSTM_UNITS', 'VOCAB_SIZE', 'NUM_TAGS', 'nn_model', 'glossary']\n",
    "for var_name in required_globals:\n",
    "    if var_name not in globals():\n",
    "        print(f\"Warning: Global variable or model '{var_name}' may not be initialized. Please run previous cells.\")\n",
    "    elif var_name == 'nn_model' and globals().get('nn_model') is None:\n",
    "        print(f\"Warning: Global 'nn_model' is None. Pipeline will attempt to build a dummy one.\")\n",
    "print(\"Checks complete.\")\n",
    "\n",
    "sample_russian_paragraph = ( # Using a multi-line string for a slightly longer text\n",
    "    \"Современные методы анализа данных играют ключевую роль в научных исследованиях и бизнес-аналитике. \"\n",
    "    \"Машинное обучение, как одно из направлений искусственного интеллекта, предлагает мощные инструменты для обработки больших объемов информации. \"\n",
    "    \"Нейронные сети и глубокое обучение позволяют выявлять сложные закономерности в данных. \"\n",
    "    \"Для эффективного анализа текста важно использовать качественную предобработку, включая лемматизацию и удаление стоп-слов. \"\n",
    "    \"Результаты такого анализа могут быть представлены в виде облака слов или частотных диаграмм.\"\n",
    ")\n",
    "\n",
    "run_term_extraction_pipeline(sample_russian_paragraph)\n",
    "\n",
    "print(\"\\n--- Second Pipeline Run with Different Text (Demonstrating Glossary Reset) ---\")\n",
    "sample_short_text = \"Искусственный интеллект это технология будущего. Анализ текста важен.\"\n",
    "run_term_extraction_pipeline(sample_short_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
